{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cPickle as p\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import word2vec\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Lab, Part I\n",
    "\n",
    "Welcome to the first lab of 6.S191!\n",
    "\n",
    "### Administrivia\n",
    "Things to install:\n",
    "- [tensorflow](https://www.tensorflow.org/get_started/os_setup)\n",
    "- [word2vec](https://github.com/danielfrg/word2vec)\n",
    "\n",
    "### Lab Objectives:\n",
    "-  Learn Machine Learning methodology basics (train/dev/test sets)\n",
    "-  Learn some Natural Language Processing basics (word embeddings with word2vec) \n",
    "-  Learn the basics of tensorflow, build your first deep neural nets (MLP, RNN) and get results!\n",
    "\n",
    "\n",
    "\n",
    "And we'll be doing all of this in te context of Twitter sentiment analysis. Given a tweet like:\n",
    "```\n",
    "omg 6.S196 is so cool #deeplearning #mit\n",
    "```\n",
    "We want an algorithm to label this tweet as positive or negative. It's intractable to try to solve this task via some lexical rules, so instead, we're going to use deep learning to embed these tweets into some deep latent space where distinguishing between the two is realtively simple.\n",
    "\n",
    "## Machine Learning Basics\n",
    "Given some dataset with tweets $X$, and sentiments $Y$, we want to learn a function $f$, such that $Y = f(X)$.\n",
    "In our context, $f$ is deep neural network parameterized by some network weights $\\Theta$, and  we're going to do our learning via gradient descent. \n",
    "\n",
    "### Objective Function\n",
    "To start, we need someway to measure how good our $f$ is, so we can take a gradient in respective to that performance and move in the right direction. We call this performance evaluation our Loss function, L , and this is something we want to minimize. \n",
    "\n",
    "Since we are doing classification (pos vs neg), a common loss function  to use is cross entropy.\n",
    "$$L( \\Theta ) = - \\Sigma_i ( f(x_i)*log(y_i) + (1-f(x_i))log(1-y_i) ) $$ where $f(x)$ is the probablity a tweet $x$ is positive, which we want to be 1 given it's postive and 0 given that it's negative and $y$ is the correct answer. We can access this function with `tf.nn.sigmoid_cross_entropy_with_logits`, which will come handy in code. Given that $f$ is parameterized by $\\Theta$, we can take the gradient $\\frac{dL}{d\\Theta}$, and we learn by updating our parameters to minimize the loss.\n",
    "\n",
    "Note that this loss is 0 if the prediction is correct and very large if we predict something has 0 probablity of being positive when it is positive.\n",
    "\n",
    "\n",
    "### Methodology\n",
    "To measure how well we're doing, we can't just look at how well our model performs on it's training data. It could be just memorizing the training data and perform terribly on data it hasn't seen before. To really measure how $f$ performs in the wild, we need to present it with unseen data, and we can tune our hyper-parameters (like learning rate, num layers etc.) over this first unseen set, which we call our development (or validation) set. However, given that we optimized our hyper-parameters to the development set, to get a true fair assesment of the model, we test it in respect to a held-out test set at the end, and generaly report those numbers.\n",
    "\n",
    "In summary:\n",
    "Namely, we training on one set, i.e. a training set,\n",
    "evaluate and tune our hyper paremeters in regards to our performance on the dev set,\n",
    "and report finals results on a completely heldout test set. \n",
    "\n",
    "Let's load these now, this ratio of sizes if fairly standard.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 20000, 20000)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainSet = p.load( open('data/train.p','rb'))\n",
    "devSet = p.load( open('data/dev.p','rb'))\n",
    "testSet = p.load( open('data/test.p','rb'))\n",
    "\n",
    "## Let's look at the size of what we have here. Note, we could use a much larger train set, \n",
    "## but we keep it mid-size so you can run this whole thing off your laptop\n",
    "len(trainSet), len(devSet), len(testSet)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP Basics\n",
    "The first question we need to address is how do we represent a tweet? how do we represent a word?\n",
    "One way to do this is with one_hot vectors for each word. Where a given word $w_i= [0,0,...,1,..0]$.\n",
    "However, in this representation, words like \"love\" and \"adore\" are as similar as \"love\" and \"hate\", because the cosine similarity is 0 in both cases. Another issue is that these vectors are huge in order to represent the whole vocab. To get around this issue the NLP community developed a techique called Word Embeddings. \n",
    "\n",
    "## Word2Vec\n",
    "\n",
    "The basic idea is we represent a word with a vector $\\phi$ by the context the word  appears in. By training a neural network to predict the context of words across a large training set, we can use the weights of that neural networks to get a dense, and useful representation that captures context. Word Embeddings capture all kinds of useful semantic relationships. For example, one cool emergent property is $ \\phi(king) - \\phi(queen) = \\phi(man) - \\phi(woman)$. To learn more about the magic behind word embeddings we recommend Chris Colahs \"[blog post](https://colah.github.io/posts/2014-07-NLP-RNNs-Representations/)\". A common tool for generating Word Embeddings is word2vec, which is what we'll be using today."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finally back twitterberry messed up my phone lets try this one out padtoken padtoken padtoken padtoken padtoken padtoken padtoken padtoken\n",
      "Starting training using file data/trainSentences.p\n",
      "Vocab size: 7597\n",
      "Words in train file: 1326930\n",
      "Alpha: 0.000042  Progress: 99.99%  Words/thread/sec: 534.18k  [u'</s>' u'padtoken' u'unktoken' ..., u'fart' u'nonstop' u'gue']\n"
     ]
    }
   ],
   "source": [
    "## Note, these tweets were preprocessings to remove non alphanumeric chars, replace unfrequent words, and padded to same length.\n",
    "## Note, we're going to train our embeddings on only our train set in order to keep our dev/test tests fair \n",
    "trainSentences = [\" \".join(tweetPair[0]) for tweetPair in trainSet]\n",
    "print trainSentences[0]\n",
    "p.dump(trainSentences, open('data/trainSentences.p','wb'))\n",
    "## Word2vec module expects a file containing a list of strings, a target to store the model, and then the size of the\n",
    "## embedding vector\n",
    "word2vec.word2vec('data/trainSentences.p','data/emeddings.bin', 100, verbose=True)\n",
    "\n",
    "w2vModel = word2vec.load('data/emeddings.bin')\n",
    "print w2vModel.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding for the word fun [ 0.07733155  0.02094666 -0.07056687 -0.15297188  0.01215174  0.11059642\n",
      "  0.09173469 -0.12253209  0.00565852  0.04393772  0.09863693  0.01615567\n",
      " -0.07865892  0.02123738 -0.01868444 -0.17429203  0.09889079  0.06510748\n",
      " -0.07211083  0.01970444 -0.01176764 -0.05357672 -0.02595131  0.20702414\n",
      "  0.0677708   0.13511761 -0.00696134 -0.0004319  -0.10437749 -0.02250373\n",
      "  0.03246941 -0.01882845  0.1492079   0.01068045 -0.00154801 -0.1346727\n",
      "  0.01907005 -0.04524891  0.02317233  0.01590484 -0.04804752  0.12876496\n",
      " -0.13109675 -0.18742943 -0.00188511  0.11791512 -0.04116412  0.05891943\n",
      "  0.02383982 -0.16589837 -0.15927446 -0.20485222  0.1233143   0.08280163\n",
      "  0.05593759  0.01914807  0.07273387 -0.09362626  0.14119175  0.02783444\n",
      " -0.05839921 -0.11627347 -0.01936541 -0.05409243 -0.02237922  0.09933708\n",
      "  0.08858543  0.08565131  0.20648398  0.20813093 -0.04754683  0.0907145\n",
      "  0.11533751 -0.04985762  0.13339184  0.00606171 -0.0988684   0.02023486\n",
      " -0.06113478  0.03745814  0.02086202 -0.0949152   0.05455122  0.15964673\n",
      "  0.15932994  0.15959385  0.05743412  0.03079876  0.1881761  -0.11659756\n",
      "  0.1144704   0.12017471 -0.10377464 -0.03360411  0.02907902  0.00385278\n",
      " -0.07648493  0.20594023 -0.22257948  0.13107707]\n"
     ]
    }
   ],
   "source": [
    "## Each word looks something like represented by a 100 dimension vector like this\n",
    "print \"embedding for the word fun\", w2vModel['fun']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets look at the words most similar to the word \"fun\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'great' u'nice' u'lovely' u'busy' u'wonderful' u\"fun'\" u'gr8' u'boring'\n",
      " u'blast' u'gone']\n"
     ]
    }
   ],
   "source": [
    "indices, cosineSim = w2vModel.cosine('fun')\n",
    "print w2vModel.vocab[indices]\n",
    "\n",
    "word_embeddings = w2vModel.vectors\n",
    "vocab_size = len(w2vModel.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to play around here test the properties of your embeddings, how they cluster etc. In the interest of time, we're going to move on straight to models.\n",
    " \n",
    "Now in order to use these embeddings, we have to represent each tweet as a list of indices into the embedding matrix.\n",
    "This preprocessing code is available in processing.py if you are interested. \n",
    " \n",
    "## Tensorflow Basics\n",
    "Tensorflow is a hugely popular library for building neural nets. The general workflow in building models in tensorflow is as follows:\n",
    "- Specify a computation graph (The struture and computations of your neural net)\n",
    "- Use your session to feed data into the graph and fetch things from the graph (like the loss, and train operation)\n",
    "Inside the graph, we put our neural net, our loss function, and our optimizer and once this is constructed, we can feed in the data, fetch the loss and the train op to train it.\n",
    "\n",
    "Here is a toy example putting 2 and 2 together, and initializing some random weight matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "[[-0.4599281  -0.03895351]\n",
      " [ 0.5871641  -0.15118274]]\n"
     ]
    }
   ],
   "source": [
    "session = tf.Session()\n",
    "# 1.BUILD GRAPH\n",
    "# Set placeholders with a type for data you'll eventually feed in (like tweets and sentiments)\n",
    "a = tf.placeholder(tf.int32)\n",
    "b = tf.placeholder(tf.int32)\n",
    "# Set up variables, like weight matrices. \n",
    "# Using tf.get_variable, specify the name, shape, type and initliazer of the variable.\n",
    "W = tf.get_variable(\"ExampleMatrix\", [2, 2], tf.float32, tf.random_normal_initializer(stddev=1.0 / 2))\n",
    "# Set up the operations you need, like matrix multiplications, non-linear layers, and your loss function minimizer\n",
    "c = a*b\n",
    "# 2.RUN GRAPH\n",
    "# Initialize any variables you have (just W in this case)\n",
    "tf.global_variables_initializer().run(session=session)\n",
    "# Specify the values tensor you want returned, and ops you want run\n",
    "fetch = {'c':c, 'W':W}\n",
    "# Fill in the place holders\n",
    "feed_dict = {\n",
    "    a: 2,\n",
    "    b: 2,\n",
    "}\n",
    "# Run and get back fetch filled in\n",
    "results = session.run( fetch, feed_dict = feed_dict)\n",
    "\n",
    "print( results['c'])\n",
    "print( results['W'])\n",
    "# Close session\n",
    "session.close()\n",
    "# Reset the graph so it doesn't get in the way later\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Building an MLP\n",
    "\n",
    "MLP or Multi-layer perceptron is a basic archetecture where where we multiply our representation with some matrix `W` and add some bias `b` and then apply some nonlineanity like `tanh` at each layer. Layers are fully connected to the next. As the network gets deeper, it's expressive power grows exponentially and so they can draw some pretty fancy decision boundaries. In this exercise, you'll build your own MLP, with 1 hidden layer (layer that isn't input or output), with 100 dimensions.\n",
    "\n",
    "To make training more stable and efficient, we'll do this we'll actually evalaute 20 tweets at a time, and take gradients and respect to the loss on the 20. We call this idea training with mini-batches.\n",
    "### Defining the Graph\n",
    "#### Step 1: Placeholders, Variables with specified shapes\n",
    "- Let start off with placeholders for our tweets, and lets use a minibatch of size 20.\n",
    "Remember each tweet is will be represented as a vector of `sentence length` (20) word_ids , and since we are packing `mini-batch size` number of tweets in the graph a time tweets per iteration, we need a matrix of `minibatch * sentence length `. Feel free to check out the placeholder api [here](https://www.tensorflow.org/api_docs/python/io_ops/placeholders#placeholder)\n",
    "- Set up a placeholder for your labels, namely the `mini-batch size` length vector of sentiments.\n",
    "- Set up a placeholder for our pretrained word embeddings. This will take shape `vocab_size * embedding_size`\n",
    "- Set up a variable for your weight matrix, and bias. Check out the variable api [here](https://www.tensorflow.org/api_docs/python/state_ops/variables) Let's use a hidden dimension size of 100 (so 100 neurons in the next layer) \n",
    "For the Weight matrix, use tf.random_normal_initializer(stddev=1.0 / hidden_dim_size), as this does something called symetry breaking and keeps the neural network from getting stuck at the start.\n",
    "For the bias vector, use tf.constant_initializer(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TODO'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"TODO\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Putting in the Operations \n",
    "- Load the word embeddings for the word ids. You can do this using tf.nn.embedding_lookup. Remember to use your embeddings placeholder. You should end up with a Tensor of dimensions `batch_size * sentence_length * embedding size`.\n",
    "- To represent a whole tweet, let's use a neural bag of words. This means we represent each tweet by the words that occur in it; it's a basic representation but gets us pretty far. To do this in a neural way, we can just average the embeddings in the tweet, leaving a single vector of `embedding size` for each tweet. You should end up with a Tensor of dimensions `batch_size * embedding size`.\n",
    "- Apply projection to the hidden layer of size 100 (ie. multiply the input by a weight vector and add a bias )\n",
    "- Apply a nonlinearity like `tf.tanh`\n",
    "- Project this to the output layer of size 1 (ie. multiply the input by a wieght vector and add a bias). Put this in a python variable called logits.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TODO'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"TODO\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Set up loss function, and optimizer to minimize it. We'll be using Adam as our optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'logits' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-535bd271ad26>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m## Make sure to call your output embedding logits, and your sentiments placeholder sentiments in python\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid_cross_entropy_with_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentiments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdamOptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1e-2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'logits' is not defined"
     ]
    }
   ],
   "source": [
    "## Make sure to call your output embedding logits, and your sentiments placeholder sentiments in python\n",
    "loss = tf.nn.sigmoid_cross_entropy_with_logits(logits, sentiments)\n",
    "loss = tf.reduce_sum(loss)\n",
    "optimizer = tf.train.AdamOptimizer(1e-2).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Graph\n",
    "#### Step 3: Set up training, and fetch optimizer at each iteration to train the model\n",
    "- First initialize all variables as in the toy example\n",
    "- Sample 20 random tweet,sentiment pairs for our feed_dict dictionary. Remember to feed in the embedding matrix.\n",
    "- fetch dictionary, the ops we want to run and tensors we want back\n",
    "- Execute this many times to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'minibatch_size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-8513d585708b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mtrainTweet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m  \u001b[0;34m[\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrainSet\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mminibatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mtrainLabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrainSet\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mminibatch_size\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'minibatch_size' is not defined"
     ]
    }
   ],
   "source": [
    "trainSet = p.load( open('data/trainTweets_preprocessed.p','rb'))\n",
    "random.shuffle(trainSet)\n",
    "\n",
    "\" TODO Init vars\"\n",
    "\n",
    "losses = []\n",
    "for i in range(5000):\n",
    "    trainTweet = np.array(  [ t[0] for t in trainSet[i: i+ minibatch_size]])\n",
    "    trainLabels = np.array( [int(t[1]) for t in trainSet[i: i+ minibatch_size] ])\n",
    "    \n",
    "    results = \"TODO, run graph with data\"\n",
    "    losses.append(results['loss'])\n",
    "    if i % 500 == 0:\n",
    "        print(\"Iteration\",i,\"Loss\", sum(losses[-500:-1])/500. if i > 0 else losses[-1])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Check validation results, and tune\n",
    "- Try running the graph on validation data, without fetching the train op.\n",
    "- See how the results compare. If the train loss is much lower than the development loss, we may be overfitting. If the train loss is still high, try experimenting with the model archetecture to increase it's capacity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'minibatch_size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-26320b50eed9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20000\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mvalTweet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m  \u001b[0;34m[\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalidationSet\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mminibatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mvalLabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalidationSet\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mminibatch_size\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'minibatch_size' is not defined"
     ]
    }
   ],
   "source": [
    "validationSet = p.load( open('data/devTweets_preprocessed.p','rb'))\n",
    "random.shuffle(validationSet)\n",
    "\n",
    "losses = []\n",
    "for i in range(20000/20):\n",
    "    valTweet = np.array(  [ t[0] for t in validationSet[i: i+ minibatch_size]])\n",
    "    valLabels = np.array( [int(t[1]) for t in validationSet[i: i+ minibatch_size] ])\n",
    "\n",
    "    results = \"TODO\" \n",
    "    losses.append(results['loss'])\n",
    "print(\"Dev Loss\", sum(losses)*1./len(losses))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Future Steps:\n",
    "Things to try on your own:\n",
    "- Adding in a tensor for accuracy, and log it at each step.\n",
    "- Iterate over whole validation dataset to get more stable validation score\n",
    "- Try tensorboard and graphing accuracy over both sets time.\n",
    "- experiment with different archetectures that maximize validation score. Maybe bag of words, which doesn't distinguish between \"bad not good\" and \"good not bad\" isn't a good enough representation. \n",
    "- test it on the test data\n",
    "- Do the RNN tutorial!\n",
    "\n",
    "# Solutions!\n",
    "Do not look unless you really have to. Ask TA's for help first. Fight for the intuition, you'll get more out of it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Iteration', 0, 'Loss', 13.865759)\n",
      "('Iteration', 500, 'Loss', 13.850158506393432)\n",
      "('Iteration', 1000, 'Loss', 13.583849094390869)\n",
      "('Iteration', 1500, 'Loss', 12.831425987243652)\n",
      "('Iteration', 2000, 'Loss', 12.317071950912476)\n",
      "('Iteration', 2500, 'Loss', 11.794303876876832)\n",
      "('Iteration', 3000, 'Loss', 12.680396286010742)\n",
      "('Iteration', 3500, 'Loss', 12.530222269058228)\n",
      "('Iteration', 4000, 'Loss', 12.272984921455384)\n",
      "('Iteration', 4500, 'Loss', 12.515208539962769)\n",
      "('Dev Loss', 12.61152536869049)\n"
     ]
    }
   ],
   "source": [
    "# Step 1:\n",
    "tf.reset_default_graph()\n",
    "session = tf.Session()\n",
    "\n",
    "\n",
    "minibatch_size = 20\n",
    "tweet_length = 20\n",
    "embedding_size = 100\n",
    "hidden_dim_size = 100\n",
    "output_size = 1\n",
    "init_bias = 0\n",
    "\n",
    "tweets          = tf.placeholder(tf.int32, shape=[minibatch_size,tweet_length])\n",
    "sentiments      = tf.placeholder(tf.float32, shape=[minibatch_size])\n",
    "embeddingMatrix = tf.placeholder(tf.float32, shape =[vocab_size, embedding_size] )\n",
    "W_hidden = tf.get_variable(\"W_hidden\", [embedding_size, hidden_dim_size], tf.float32, tf.random_normal_initializer(stddev=1.0 / hidden_dim_size))\n",
    "b_hidden = tf.get_variable(\"b_hidden\", [hidden_dim_size], initializer=tf.constant_initializer(init_bias))\n",
    "W_output = tf.get_variable(\"W_output\", [hidden_dim_size, output_size], tf.float32, tf.random_normal_initializer(stddev=1.0 / hidden_dim_size))\n",
    "b_output = tf.get_variable(\"b_output\", [output_size], initializer=tf.constant_initializer(init_bias))\n",
    "\n",
    "# Step 2:\n",
    "tweet_embedded =  tf.nn.embedding_lookup(embeddingMatrix, tweets)\n",
    "averagedTweets = tf.reduce_mean(tweet_embedded, axis=1)\n",
    "hidden_proj = tf.matmul( averagedTweets, W_hidden) + b_hidden\n",
    "non_linearity = tf.nn.tanh(hidden_proj)\n",
    "logits = tf.matmul( non_linearity,  W_output)+ b_output\n",
    "logits = tf.reshape(logits, shape=[minibatch_size])\n",
    "\n",
    "## Make sure to call your output embedding logits, and your sentiments placeholder sentiments in python\n",
    "loss = tf.nn.sigmoid_cross_entropy_with_logits(logits, sentiments)\n",
    "loss = tf.reduce_sum(loss)\n",
    "optimizer = tf.train.AdamOptimizer().minimize(loss)\n",
    "\n",
    "# Step 3:\n",
    "trainSet = p.load( open('data/trainTweets_preprocessed.p','rb'))\n",
    "random.shuffle(trainSet)\n",
    "\n",
    "tf.global_variables_initializer().run(session=session)\n",
    "\n",
    "losses = []\n",
    "for i in range(5000):\n",
    "    trainTweet = np.array(  [ t[0] for t in trainSet[i: i+ minibatch_size]])\n",
    "    trainLabels = np.array( [int(t[1]) for t in trainSet[i: i+ minibatch_size] ])\n",
    "    \n",
    "    feed_dict = {\n",
    "        embeddingMatrix: word_embeddings,\n",
    "        tweets: trainTweet,\n",
    "        sentiments: trainLabels\n",
    "    }\n",
    "    fetch = {\n",
    "        'loss': loss,\n",
    "        'trainOp': optimizer\n",
    "    }\n",
    "    results = session.run(fetch, feed_dict=feed_dict)\n",
    "    losses.append(results['loss'])\n",
    "    if i % 500 == 0:\n",
    "        print(\"Iteration\",i,\"Loss\", sum(losses[-500:-1])/500. if i > 0 else losses[-1])\n",
    "    \n",
    "\n",
    "# Step 4:\n",
    "validationSet = p.load( open('data/devTweets_preprocessed.p','rb'))\n",
    "random.shuffle(validationSet)\n",
    "\n",
    "losses = []\n",
    "for i in range(20000/20):\n",
    "    valTweet = np.array(  [ t[0] for t in validationSet[i: i+ minibatch_size]])\n",
    "    valLabels = np.array( [int(t[1]) for t in validationSet[i: i+ minibatch_size] ])\n",
    "    feed_dict = {\n",
    "        embeddingMatrix: word_embeddings,\n",
    "        tweets: valTweet,\n",
    "        sentiments: valLabels\n",
    "    }\n",
    "    fetch = {\n",
    "        'loss': loss,\n",
    "    }\n",
    "    results = session.run(fetch, feed_dict=feed_dict)\n",
    "    losses.append(results['loss'])\n",
    "print(\"Dev Loss\", sum(losses)*1./len(losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Part II: RNN Sentiment Classifier\n",
    "\n",
    "In the previous lab, you built a tweet sentiment classifier based on Bag-Of-Words features. Now we ask you to improve this model by representing it as a sequence of words.\n",
    "\n",
    "## Step 1: Build an LSTM Encoder\n",
    "\n",
    "A classifier requires the input feature vector to be of fixed size, while sentences are of different lengths. Thus, we need a model (called as **encoder**) to transform a sentence to a fixed size vector. This could be done by a recurrent neural net (RNN), by taking the last hidden state of LSTM encoder as the feature vector. We could then build a linear (or a multi-layer) network upon it to perform a classifier.\n",
    "\n",
    "### Step 1.1 Embedding Lookup Layer\n",
    "\n",
    "At input layer, words are represented by their ID (one-hot vector). Instead of using pre-trained word vector, this time we would let RNN to learn the embedding by itself. You could use the tensorflow API function [embedding-lookup](https://www.tensorflow.org/api_docs/python/nn/embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.2 LSTM Layer\n",
    "\n",
    "Now we have the embedding layer, we can build LSTM layer upon it. It requires 4 steps:\n",
    "1. Create a LSTM Cell using [BasicLSTMCell](https://www.tensorflow.org/api_docs/python/rnn_cell/rnn_cells_for_use_with_tensorflow_s_core_rnn_methods)\n",
    "2. Let's say you have a `lstm_cell` object, declare initial state vector by calling `lstm_cell.zero_state()`.\n",
    "3. Create a RNN Layer using [dynamic-rnn](https://www.tensorflow.org/api_docs/python/nn/recurrent_neural_networks), get the final state of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.3 Classification Layer\n",
    "\n",
    "Now you have a fixed-size vector for sentences, build a classification layer the same as previous. Declare the cross-entropy loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.4 Training\n",
    "\n",
    "Now you need to feed the network with training data, and optimize it. We have already **pad** sentences in the batch to the same length. If you wanna do this by yourself, you could put some special \"<PAD>\" tokens before each sentences so that they are in the same length. (Don't put them in the back because it would be harder to get the final hidden state of each sentence!) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2605\n",
      "0.22375\n",
      "0.229333333333\n",
      "0.2134375\n",
      "0.2001\n",
      "0.183583333333\n",
      "0.172678571429\n",
      "0.16321875\n",
      "0.156805555556\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-667e9a4fd716>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mfeed_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mtweets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtrainTweet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtrainLabels\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_err\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_err\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m     \u001b[0merr_rate\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbatch_err\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m200\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/wengong/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 766\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    767\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/wengong/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    962\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 964\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    965\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/wengong/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1014\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1015\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/Users/wengong/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1019\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1020\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1022\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/wengong/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1001\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1002\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1003\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1004\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1005\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import math\n",
    "import cPickle as p\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "#build RNN model\n",
    "batch_size = 20\n",
    "hidden_size = 100\n",
    "vocab_size = 7597\n",
    "\n",
    "tf.reset_default_graph()\n",
    "session = tf.Session()\n",
    "\n",
    "def lookup_table(input_, vocab_size, output_size, name):\n",
    "    with tf.variable_scope(name):\n",
    "        embedding = tf.get_variable(\"embedding\", [vocab_size, output_size], tf.float32, tf.random_normal_initializer(stddev=1.0 / math.sqrt(output_size)))\n",
    "    return tf.nn.embedding_lookup(embedding, input_)\n",
    "\n",
    "def linear(input_, output_size, name, init_bias=0.0):\n",
    "    shape = input_.get_shape().as_list()\n",
    "    with tf.variable_scope(name):\n",
    "        W = tf.get_variable(\"Matrix\", [shape[-1], output_size], tf.float32, tf.random_normal_initializer(stddev=1.0 / math.sqrt(shape[-1])))\n",
    "    if init_bias is None:\n",
    "        return tf.matmul(input_, W)\n",
    "    with tf.variable_scope(name):\n",
    "        b = tf.get_variable(\"bias\", [output_size], initializer=tf.constant_initializer(init_bias))\n",
    "    return tf.matmul(input_, W) + b\n",
    "\n",
    "tweets = tf.placeholder(tf.int32, [batch_size, None])\n",
    "labels = tf.placeholder(tf.float32, [batch_size])\n",
    "\n",
    "embedding = lookup_table(tweets, vocab_size, hidden_size, name=\"word_embedding\")\n",
    "lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(hidden_size)\n",
    "init_state = lstm_cell.zero_state(batch_size, tf.float32)\n",
    "_, final_state = tf.nn.dynamic_rnn(lstm_cell, embedding, initial_state=init_state)\n",
    "sentiment = linear(final_state[1], 1, name=\"output\")\n",
    "\n",
    "sentiment = tf.squeeze(sentiment, [1])\n",
    "loss = tf.nn.sigmoid_cross_entropy_with_logits(sentiment, labels)\n",
    "loss = tf.reduce_mean(loss)\n",
    "prediction = tf.to_float(tf.greater_equal(sentiment, 0.5))\n",
    "pred_err = tf.to_float(tf.not_equal(prediction, labels))\n",
    "pred_err = tf.reduce_sum(pred_err)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer().minimize(loss)\n",
    "\n",
    "tf.global_variables_initializer().run(session=session)\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "trainSet = p.load( open('data/trainTweets_preprocessed.p','rb'))\n",
    "err_rate = 0.0\n",
    "for i in range(5000):\n",
    "    trainTweet = np.array(  [ t[0] for t in trainSet[i: i+ batch_size]])\n",
    "    trainLabels = np.array( [int(t[1]) for t in trainSet[i: i+ batch_size]])\n",
    "    if len(trainTweet) != batch_size:\n",
    "        continue\n",
    "    feed_map = {tweets:trainTweet, labels:trainLabels}\n",
    "    _, batch_err = session.run([optimizer, pred_err], feed_dict=feed_map)\n",
    "    err_rate += batch_err / batch_size\n",
    "    if i % 200 == 0 and i > 0:\n",
    "        print err_rate / i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "name": "_merged"
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
